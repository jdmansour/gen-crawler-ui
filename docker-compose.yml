services:
  web:
    image: jdmansour/generic-crawler-web:${DC_IMAGE_VERSION:-main}
    build: ui
    ports:
      - "8000:8000"
    volumes:
      - "./database:/app/database"
    depends_on:
      - scrapyd
      - redis
    environment:
      - SCRAPYD_URL=http://scrapyd:6800
      - DB_PATH=/app/database/db.sqlite3
      - REDIS_URL=redis://redis:6379/0
    command: >
      sh -c "python manage.py migrate &&
             gunicorn crawler_ui.wsgi --bind 0.0.0.0:8000"
  metadataapi:
    image: jdmansour/generic-crawler-metadataapi:${DC_IMAGE_VERSION:-main}
    build:
      context: .
      dockerfile: metadataapi.Dockerfile
    ports:
      - "8081:8081"
    depends_on:
      - headless_chrome
    environment:
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      - "Z_API_KEY=${Z_API_KEY?}"
      - "GENERIC_CRAWLER_USE_LLM_API=${GENERIC_CRAWLER_USE_LLM_API?}"
      - "GENERIC_CRAWLER_LLM_API_KEY=${GENERIC_CRAWLER_LLM_API_KEY?}"
      - "GENERIC_CRAWLER_LLM_API_BASE_URL=${GENERIC_CRAWLER_LLM_API_BASE_URL}"
      - "GENERIC_CRAWLER_LLM_MODEL=${GENERIC_CRAWLER_LLM_MODEL}"
      - "API_KEY=${METADATAAPI_API_KEY?}"
  scrapyd:
    image: jdmansour/generic-crawler-scrapyd:${DC_IMAGE_VERSION:-main}
    build:
      context: .
      dockerfile: scraper.Dockerfile
    # # needed so we can install the playwright python package in the container
    # platform: linux/amd64
    ports:
      - "6800:6800"
    volumes:
      - "./database:/app/database"
      - "./scrapyd:/var/lib/scrapyd"
    depends_on:
      - headless_chrome
      - redis
    environment:
      - DB_PATH=/app/database/db.sqlite3
      - GENERIC_CRAWLER_DB_PATH=/app/database/db.sqlite3
      - REDIS_URL=redis://redis:6379/0
      # For the generic crawler
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      # optional keyword args, e.g. cleanrun=true
      - "ARGS=${ARGS}"
      - "MODE=edu-sharing"
      - "LOG_LEVEL=${LOG_LEVEL:-INFO}"
      - "EDU_SHARING_BASE_URL=${EDU_SHARING_BASE_URL?}"
      - "EDU_SHARING_USERNAME=${EDU_SHARING_USERNAME?}"
      - "EDU_SHARING_PASSWORD=${EDU_SHARING_PASSWORD?}"
      - "EDU_SHARING_PERMISSION_CONTROL=${EDU_SHARING_PERMISSION_CONTROL:-false}"
      - "Z_API_KEY=${Z_API_KEY?}"
      - "GENERIC_CRAWLER_USE_LLM_API=${GENERIC_CRAWLER_USE_LLM_API?}"
      - "GENERIC_CRAWLER_LLM_API_KEY=${GENERIC_CRAWLER_LLM_API_KEY?}"
      - "GENERIC_CRAWLER_LLM_API_BASE_URL=${GENERIC_CRAWLER_LLM_API_BASE_URL}"
      - "GENERIC_CRAWLER_LLM_MODEL=${GENERIC_CRAWLER_LLM_MODEL}"
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

  headless_chrome:
    image: ghcr.io/browserless/chrome@sha256:88ad4d675814dc8c3e5cfaa79bccec2253c7ef04abfa6a55ecc4d605090ff954
    platform: linux/amd64
    restart: always
    environment:
      - TIMEOUT=60000
    ports:
      - "127.0.0.1:3000:3000"
    # networks:
    #   - scrapy

volumes:
  redis_data:
