services:
  web:
    build: ui
    ports:
      - "8000:8000"
    volumes:
      - "./database:/app/database"
    environment:
      - SCRAPYD_URL=http://scrapyd:6800
      - DB_PATH=/app/database/db.sqlite3
  metadataapi:
    build:
      context: .
      dockerfile: metadataapi.Dockerfile
    ports:
      - "8081:8081"
    environment:
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      - "Z_API_KEY=${Z_API_KEY?}"
      - "GENERIC_CRAWLER_LLM_API_KEY=${GENERIC_CRAWLER_LLM_API_KEY}"
      - "GENERIC_CRAWLER_USE_LLM_API=${GENERIC_CRAWLER_USE_LLM_API?}"
  scrapyd:
    build:
      context: .
      dockerfile: scraper.Dockerfile
    # # needed so we can install the playwright python package in the container
    # platform: linux/amd64
    ports:
      - "6800:6800"
    volumes:
      - "./database:/app/database"
    depends_on:
      - headless_chrome
    environment:
      - DB_PATH=/app/database/db.sqlite3
      - GENERIC_CRAWLER_DB_PATH=/app/database/db.sqlite3
      # For the generic crawler
      - "PYPPETEER_WS_ENDPOINT=ws://headless_chrome:3000"
      - "PLAYWRIGHT_WS_ENDPOINT=ws://headless_chrome:3000"
      - "SPLASH_URL=http://splash:8050"
      # optional keyword args, e.g. cleanrun=true
      - "ARGS=${ARGS}"
      - "MODE=edu-sharing"
      - "LOG_LEVEL=${LOG_LEVEL:-INFO}"
      - "EDU_SHARING_BASE_URL=${EDU_SHARING_BASE_URL?}"
      - "EDU_SHARING_USERNAME=${EDU_SHARING_USERNAME?}"
      - "EDU_SHARING_PASSWORD=${EDU_SHARING_PASSWORD?}"
      - "Z_API_KEY=${Z_API_KEY?}"
      - "GENERIC_CRAWLER_LLM_API_KEY=${GENERIC_CRAWLER_LLM_API_KEY}"
      - "GENERIC_CRAWLER_USE_LLM_API=${GENERIC_CRAWLER_USE_LLM_API?}"
  headless_chrome:
    image: ghcr.io/browserless/chrome@sha256:f27f9fa0d9c2344180c0fc5af7c6ea4a1df6f2a7a3efc555de876dbea6ded7a1
    restart: always
    environment:
      - TIMEOUT=60000
    ports:
      - "127.0.0.1:3000:3000"
    # networks:
    #   - scrapy
